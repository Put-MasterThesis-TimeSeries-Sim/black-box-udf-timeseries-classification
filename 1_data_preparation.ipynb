{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from datetime import datetime\n",
    "entry_directory = \"Raw\"\n",
    "prepared_directory = \"Prepared\"\n",
    "organised_directory = \"Organised\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum CPU and RAM usage for specific timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sums(file_path):\n",
    "\n",
    "    node_data_df = pd.read_csv(file_path, dtype = {'timestamp' : 'string', 'PID' : 'int', 'CPU': 'float64', 'RAM': 'float64'})\n",
    "    node_data_df['timestamp'] = node_data_df['timestamp'].apply(lambda x: x if len(x.split(\".\")) > 1 else x + \".000000\" )\n",
    "    node_data_df['epoch'] = node_data_df['timestamp'].apply(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\") - datetime(1970, 1, 1)).total_seconds())\n",
    "    min_timestamp = node_data_df['epoch'].min()\n",
    "    node_data_df['epoch'] = node_data_df['epoch'].apply(lambda x: x - min_timestamp)\n",
    "    node_data_df.drop('PID', axis='columns', inplace=True)\n",
    "    node_data_df.drop('timestamp', axis='columns', inplace=True)\n",
    "    node_data_df = node_data_df.groupby(\"epoch\").sum()\n",
    "\n",
    "    return node_data_df\n",
    "\n",
    "\n",
    "for root, _, files in os.walk(f\".\\{entry_directory}\"):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            if re.search(\"\\d\\d_\\d\\d_\\d\\d\\d\\d_\\d\\d_\\d\\d_\\d\\d.csv$\", full_path):\n",
    "                if os.path.exists(full_path.replace(entry_directory, prepared_directory)):\n",
    "                    continue\n",
    "                calculated_dataframe_to_save = calculate_sums(full_path)\n",
    "                if not os.path.exists(root.replace(entry_directory, prepared_directory)):\n",
    "                    os.makedirs(root.replace(entry_directory, prepared_directory))\n",
    "                calculated_dataframe_to_save.to_csv(full_path.replace(entry_directory, prepared_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find worker nodes and move them to Organised directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root = f\".\\{prepared_directory}\"\n",
    "for functionTypeDirectory in os.listdir(root):\n",
    "    if os.path.isdir(os.path.join(root, functionTypeDirectory)):\n",
    "        for functionDirectory in os.listdir(os.path.join(root, functionTypeDirectory)):\n",
    "            for numberOfNodesDirectory in os.listdir(os.path.join(root, functionTypeDirectory, functionDirectory)):\n",
    "                destinationPath =  os.path.join(root, functionTypeDirectory, functionDirectory, numberOfNodesDirectory, 'source-data')\n",
    "                for nodeDirectory in os.listdir(destinationPath):\n",
    "                    path =  os.path.join(destinationPath, nodeDirectory)\n",
    "                    all_files = glob.glob(path + \"/*.csv\")\n",
    "                    dfs = []\n",
    "                    i = 1\n",
    "                    for filename in all_files:\n",
    "                        df = pd.read_csv(filename)\n",
    "                        dfs.append(df)\n",
    "                        if df['RAM'].mean() >= 2 and df['CPU'].head(15).mean() > 3:\n",
    "                            if not os.path.exists(os.path.join(destinationPath.replace(prepared_directory, organised_directory), str(i))):\n",
    "                                os.makedirs(os.path.join(destinationPath.replace(prepared_directory, organised_directory), str(i)))\n",
    "                            shutil.copyfile(filename, os.path.join(destinationPath.replace(prepared_directory, organised_directory), str(i), os.path.basename(filename)))\n",
    "                        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(entry_dir, category):\n",
    "    directory = f\"./{entry_dir}/{category}\"\n",
    "    udf_dirs = os.listdir(directory)\n",
    "    label = category\n",
    "    result_df = pd.DataFrame()\n",
    "    snapshot = 0\n",
    "    for udf in udf_dirs:\n",
    "        for root, _, files in os.walk(f\"./{directory}/{udf}\"):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "\n",
    "                node_data_df = pd.read_csv(full_path)\n",
    "                node_data_df['snapshot'] = snapshot\n",
    "                snapshot += 1\n",
    "                node_data_df[\"label\"] = label\n",
    "                node_data_df[\"udf\"] = udf\n",
    "                node_data_df[\"size\"] = full_path.split(\"\\\\\")[-4].split(\"-\")[3][0]\n",
    "                result_df = pd.concat([result_df,node_data_df])\n",
    "    result_df[[\"snapshot\", \"label\", \"udf\", \"epoch\", \"CPU\", \"RAM\", \"size\"]].to_csv(f\"{directory}/joined_{category}.csv\", index = False)\n",
    "\n",
    "labels = ['aggregation', 'filtration', 'filtration-aggregation', 'filtration-aggregation-join', 'filtration-join']\n",
    "\n",
    "for label in labels:\n",
    "    join_files(organised_directory, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize joined files for each udf type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_types = ['aggregation', 'filtration', 'filtration-aggregation', 'filtration-aggregation-join', 'filtration-join']\n",
    "full_df = pd.DataFrame()\n",
    "for udf_type in udf_types:\n",
    "    full_df = pd.concat([full_df, pd.read_csv(f\"{organised_directory}/{udf_type}/joined_{udf_type}.csv\")])\n",
    "\n",
    "full_df.CPU = (full_df.CPU-full_df.CPU.min())/(full_df.CPU.max() - full_df.CPU.min())\n",
    "full_df.RAM = (full_df.RAM-full_df.RAM.min())/(full_df.RAM.max() - full_df.RAM.min())\n",
    "for udf_type in udf_types:\n",
    "    if not os.path.exists(f\"{organised_directory}/{udf_type}\"):\n",
    "        os.makedirs(f\"{organised_directory}/{udf_type}\")\n",
    "    full_df[full_df.label == udf_type].to_csv(f\"./{organised_directory}/{udf_type}/normalized_{udf_type}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth out values in joined and normalised files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def smooth_values(entry_dir, category, window_value, file_type):\n",
    "    # reading file\n",
    "    path_to_df = f\"./{entry_dir}/{category}/{file_type}_{category}.csv\"\n",
    "    df = pd.read_csv(path_to_df)\n",
    "    cpu_df = df.groupby('snapshot')['CPU'].rolling(window=window_value, min_periods = 1).mean().to_frame()\n",
    "    ram_df = df.groupby('snapshot')['RAM'].rolling(window=window_value, min_periods = 1).mean().to_frame()\n",
    "    df['CPU'] = cpu_df.reset_index()['CPU'] \n",
    "    df['CPU'] = df['CPU'].round(2)\n",
    "    df['RAM'] = ram_df.reset_index()['RAM']\n",
    "    df['RAM'] = df['RAM'].round(2)\n",
    "    df['epoch'] = df['epoch'].round(3)\n",
    "    df.fillna(0).to_csv(f\"./{entry_dir}/{category}/{window_value}_{file_type}_smooth_{category}.csv\", index = False)\n",
    "\n",
    "labels = ['aggregation', 'filtration', 'filtration-aggregation', 'filtration-aggregation-join', 'filtration-join']\n",
    "window_sizes = [6, 12, 18]\n",
    "for window_size in window_sizes:\n",
    "    for label in labels:\n",
    "        smooth_values(organised_directory, label, window_size, \"joined\")\n",
    "        smooth_values(organised_directory, label, window_size, \"normalized\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7d812b6b0ae132ddf009a87a46d9f35cc0a6e0343f7e24aabc62c05919a2e18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}