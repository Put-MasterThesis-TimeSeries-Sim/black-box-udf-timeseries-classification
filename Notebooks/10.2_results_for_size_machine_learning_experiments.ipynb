{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zanalizowanie i wizualizacja wyników eksperymentów badających podobieństwo między wolumenami danych w ramach tych samych typów UDF\n",
    "\n",
    "Każdy z algorytmów został poddany takiej samej analizie. Wpierw liczona jest średnia dla każdego z typów algorytmów. Następnie tworzony jest boxplot sprawdzający wpływ wartości losowej na wyniki. Na końcu liczona jest macierz pomyłek dla wyników, który jest najbardziej zbliżony do wyniku średniego.\n",
    "\n",
    "Dodatkowo, na końcu wykonana jest analiza porównawcza każdego z badanych algorytmów.\n",
    "### Deklaracja zmiennych użytkowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from plotnine import *\n",
    "\n",
    "RESULTS_PATH = \"./../MachineLearning/results/size\"\n",
    "PLOTS = \"./../Plots/MachineLearning\"\n",
    "datasets_path = [\"aggregation\", \"filtration\", \"filtration-aggregation\", \"filtration-aggregation-join\", \"filtration-join\"]\n",
    "datasets = [\"aggregation\", \"filtering\", \"filtering-aggregation\", \"filtering-aggregation-join\", \"filtering-join\"]\n",
    "name_dict = {\"aggregation\" : \"aggregation\", \"filtration\" : \"filtering\", \"filtration-aggregation\" : \"filtering-aggregation\", \"filtration-aggregation-join\" : \"filtering-aggregation-join\", \"filtration-join\" : \"filtering-join\"}\n",
    "class_names = [\"1GB\", \"2GB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorythm = \"Rocket\"\n",
    "if not os.path.exists(f\"{PLOTS}/{algorythm}/size\"):\n",
    "    os.makedirs(f\"{PLOTS}/{algorythm}/size\")\n",
    "dataframes = []\n",
    "for dataset in datasets_path:\n",
    "    path_to_files = f\"{RESULTS_PATH}/{dataset}/{algorythm}\"\n",
    "    file_names = os.listdir(f\"{RESULTS_PATH}/{dataset}/{algorythm}\")\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(f\"{path_to_files}/{file_name}\", index_col=None, header=0)\n",
    "        df.y_predict = df.y_predict.apply(lambda x: np.array(x.replace(\"\\n\", \"\").replace(\"'\", \"\")[1:-1].split(\" \")))\n",
    "        df.y_true = df.y_true.apply(lambda x: np.array(x.replace(\"\\n\", \"\").replace(\"'\", \"\")[1:-1].split(\" \")))\n",
    "        df.dataset = df.dataset.apply(lambda x: name_dict[x])\n",
    "        dataframes.append(df[[\"dataset\", \"seed\", \"accuracy_score\", \"f1_measure\", \"execution_time\", \"y_predict\", \"y_true\"]])\n",
    "rocket_df = pd.concat(dataframes, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket średnie miar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_means_df = rocket_df.groupby(\"dataset\")[[\"accuracy_score\", \"f1_measure\", \"execution_time\"]].mean().apply(lambda x: np.round(x, decimals=3))\n",
    "rocket_means_df.head()\n",
    "\n",
    "rocket_accuracy_plot = (\n",
    "    ggplot(rocket_df)\n",
    "    + geom_boxplot(aes(x='factor(dataset)', y='accuracy_score'))\n",
    "    + scale_x_discrete(labels=rocket_df.dataset.unique(), name='typ funkcji')+\n",
    "    theme(axis_text_x=element_text(rotation=90, hjust=0.4)) +\n",
    "    labs(title='Wykreś wartości trafności ROCKET.\\n Klasyfikacja rozmiaru wolumenu przetwarzanych danych.')\n",
    ")\n",
    "\n",
    "rocket_fmeasure_plot = (\n",
    "    ggplot(rocket_df)\n",
    "    + geom_boxplot(aes(x='factor(dataset)', y='f1_measure'))\n",
    "    + scale_x_discrete(labels=rocket_df.dataset.unique(), name='typ funkcji')+\n",
    "    theme(axis_text_x=element_text(rotation=90, hjust=0.4)) +\n",
    "    labs(title='Wykreś wartości F-miary ROCKET.\\n Klasyfikacja rozmiaru wolumenu przetwarzanych danych.')\n",
    ")\n",
    "print(rocket_accuracy_plot)\n",
    "rocket_accuracy_plot.save(f\"{PLOTS}/{algorythm}/size/rocket_accuracy_boxplot.pdf\", dpi=600, verbose = False)\n",
    "print(rocket_fmeasure_plot)\n",
    "rocket_fmeasure_plot.save(f\"{PLOTS}/{algorythm}/size/rocket_fmeasure_boxplot.pdf\", dpi=600, verbose = False)\n",
    "rocket_means_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rcoket macierz pomyłek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = rocket_df.copy()\n",
    "result = {}\n",
    "for dataset in datasets:    \n",
    "    testing_df[\"closest_acc\"] = np.abs(testing_df[testing_df.dataset == dataset].accuracy_score - rocket_means_df[rocket_means_df.index == dataset].accuracy_score[0])\n",
    "    local_min = testing_df[testing_df.dataset == dataset].closest_acc.min()\n",
    "    results_df = testing_df.loc[testing_df.closest_acc == local_min].head(1).copy()\n",
    "    result[dataset] = results_df\n",
    "    titles_options = [\n",
    "        (\"Macierz pomyłek:,\\n\"+\"algorytm Rocket,\\n\" +f\"zbiór danych {dataset}\", None)\n",
    "    ]\n",
    "    for title, normalize in titles_options:\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(\n",
    "            results_df.y_true.iloc[0],\n",
    "            results_df.y_predict.iloc[0],\n",
    "            display_labels=class_names,\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize,\n",
    "            xticks_rotation=\"vertical\",\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "        disp.ax_.set_ylabel(\"Faktyczna klasa\")\n",
    "        disp.ax_.set_xlabel(\"Przewidzana klasa\")\n",
    "        if normalize:\n",
    "            normalized = \"_normalized\"\n",
    "        else:\n",
    "            normalized = \"\"\n",
    "        disp.figure_.savefig(f\"{PLOTS}/{algorythm}/size/{dataset}{normalized}_confusion_matrix.pdf\", dpi=600, bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVECOTEV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorythm = \"HIVE\"\n",
    "if not os.path.exists(f\"{PLOTS}/{algorythm}/size\"):\n",
    "    os.makedirs(f\"{PLOTS}/{algorythm}/size\")\n",
    "dataframes = []\n",
    "for dataset in datasets_path:\n",
    "    path_to_files = f\"{RESULTS_PATH}/{dataset}/{algorythm}\"\n",
    "    file_names = os.listdir(f\"{RESULTS_PATH}/{dataset}/{algorythm}\")\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(f\"{path_to_files}/{file_name}\", index_col=None, header=0)\n",
    "        df.y_predict = df.y_predict.apply(lambda x: np.array(x.replace(\"\\n\", \"\").replace(\"'\", \"\")[1:-1].split(\" \")))\n",
    "        df.y_true = df.y_true.apply(lambda x: np.array(x.replace(\"\\n\", \"\").replace(\"'\", \"\")[1:-1].split(\" \")))\n",
    "        df.dataset = df.dataset.apply(lambda x: name_dict[x])\n",
    "        dataframes.append(df[[\"dataset\", \"seed\", \"accuracy_score\", \"f1_measure\", \"execution_time\", \"y_predict\", \"y_true\"]])\n",
    "hive_df = pd.concat(dataframes, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVECOTEV2 średnie miar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_means_df = hive_df.groupby(\"dataset\")[[\"accuracy_score\", \"f1_measure\", \"execution_time\"]].mean().apply(lambda x: np.round(x, decimals=3))\n",
    "hive_means_df.head()\n",
    "\n",
    "hive_accuracy_plot = (\n",
    "    ggplot(hive_df)\n",
    "    + geom_boxplot(aes(x='factor(dataset)', y='accuracy_score'))\n",
    "    + scale_x_discrete(labels=hive_df.dataset.unique(), name='typ funkcji') +\n",
    "    theme(axis_text_x=element_text(rotation=90, hjust=0.4)) +\n",
    "    labs(title='Wykreś wartości trafności HIVECOTEV2.\\n Klasyfikacja rozmiaru wolumenu przetwarzanych danych.')\n",
    ")\n",
    "\n",
    "hive_fmeasure_plot = (\n",
    "    ggplot(hive_df)\n",
    "    + geom_boxplot(aes(x='factor(dataset)', y='f1_measure'))\n",
    "    + scale_x_discrete(labels=hive_df.dataset.unique(), name='typ funkcji') +\n",
    "    theme(axis_text_x=element_text(rotation=90, hjust=0.4)) +\n",
    "    labs(title='Wykreś wartości F-miary HIVECOTEV2.\\n Klasyfikacja rozmiaru wolumenu przetwarzanych danych.')\n",
    ")\n",
    "print(hive_accuracy_plot)\n",
    "hive_accuracy_plot.save(f\"{PLOTS}/{algorythm}/size/hive_accuracy_boxplot.pdf\", dpi=600, verbose = False)\n",
    "print(hive_fmeasure_plot)\n",
    "hive_fmeasure_plot.save(f\"{PLOTS}/{algorythm}/size/hive_fmeasure_boxplot.pdf\", dpi=600, verbose = False)\n",
    "hive_means_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVECOTEV2 macierz pomyłek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = hive_df.copy()\n",
    "result = {}\n",
    "for dataset in datasets:    \n",
    "    testing_df[\"closest_acc\"] = np.abs(testing_df[testing_df.dataset == dataset].accuracy_score - hive_means_df[hive_means_df.index == dataset].accuracy_score[0])\n",
    "    local_min = testing_df[testing_df.dataset == dataset].closest_acc.min()\n",
    "    results_df = testing_df.loc[testing_df.closest_acc == local_min].head(1).copy()\n",
    "    result[dataset] = results_df\n",
    "    titles_options = [\n",
    "        (\"Macierz pomyłek:,\\n\"+\"algorytm HIVECOTEV2,\\n\" +f\"funkcja {dataset}\", None)\n",
    "    ]\n",
    "    for title, normalize in titles_options:\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(\n",
    "            results_df.y_true.iloc[0],\n",
    "            results_df.y_predict.iloc[0],\n",
    "            display_labels=class_names,\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "        disp.ax_.set_ylabel(\"Faktyczna klasa\")\n",
    "        disp.ax_.set_xlabel(\"Przewidzana klasa\")\n",
    "        if not os.path.exists(f\"{PLOTS}/{algorythm}\"):\n",
    "            os.makedirs(f\"{PLOTS}/{algorythm}\")\n",
    "        if normalize:\n",
    "            normalized = \"_normalized\"\n",
    "        else:\n",
    "            normalized = \"\"\n",
    "        disp.figure_.savefig(f\"{PLOTS}/{algorythm}/size/{dataset}{normalized}_confusion_matrix.pdf\", dpi=600, bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorythm = \"KNN\"\n",
    "if not os.path.exists(f\"{PLOTS}/{algorythm}/size\"):\n",
    "    os.makedirs(f\"{PLOTS}/{algorythm}/size\")\n",
    "dataframes = []\n",
    "for dataset in datasets_path:\n",
    "    path_to_files = f\"{RESULTS_PATH}/{dataset}/{algorythm}\"\n",
    "    file_names = os.listdir(f\"{RESULTS_PATH}/{dataset}/{algorythm}\")\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(f\"{path_to_files}/{file_name}\", index_col=None, header=0)\n",
    "        df.y_predict = df.y_predict.apply(lambda x: np.array(x.replace(\"\\n\", \"\").replace(\"'\", \"\")[1:-1].split(\" \")))\n",
    "        df.y_true = df.y_true.apply(lambda x: np.array(x.replace(\"\\n\", \"\").replace(\"'\", \"\")[1:-1].split(\" \")))\n",
    "        df[\"k\"] = df[\"seed\"]\n",
    "        df.dataset = df.dataset.apply(lambda x: name_dict[x])\n",
    "        dataframes.append(df[[\"dataset\", \"k\", \"accuracy_score\", \"f1_measure\", \"execution_time\", \"y_predict\", \"y_true\"]])\n",
    "knn_df = pd.concat(dataframes, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN-DTW średnie miar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_means_df = knn_df.groupby(\"dataset\")[[\"accuracy_score\", \"f1_measure\", \"execution_time\"]].mean().apply(lambda x: np.round(x, decimals=3))\n",
    "knn_means_df.head()\n",
    "\n",
    "knn_accuracy_plot = (\n",
    "    ggplot(knn_df)\n",
    "    + geom_boxplot(aes(x='factor(dataset)', y='accuracy_score'))\n",
    "    + geom_point(aes(x='factor(dataset)', y='f1_measure', color=\"factor(k)\"))\n",
    "    + guides(color=guide_legend(title='k'))\n",
    "    + scale_x_discrete(labels=knn_df.dataset.unique(), name='typ funkcji') +\n",
    "    theme(axis_text_x=element_text(rotation=90, hjust=0.4)) +\n",
    "    labs(title='Wykreś wartości trafności KNN-DTW.\\n Klasyfikacja rozmiaru wolumenu przetwarzanych danych.')\n",
    ")\n",
    "\n",
    "knn_fmeasure_plot = (\n",
    "    ggplot(knn_df)\n",
    "    + geom_boxplot(aes(x='factor(dataset)', y='f1_measure'))\n",
    "    + geom_point(aes(x='factor(dataset)', y='f1_measure', color=\"factor(k)\"))\n",
    "    + guides(color=guide_legend(title='k'))\n",
    "    + scale_x_discrete(labels=knn_df.dataset.unique(), name='typ funkcji') \n",
    "    + theme(axis_text_x=element_text(rotation=90, hjust=0.4)) +\n",
    "    labs(title='Wykreś wartości F-miary KNN-DTW.\\n Klasyfikacja rozmiaru wolumenu przetwarzanych danych.')\n",
    ")\n",
    "print(knn_accuracy_plot)\n",
    "knn_accuracy_plot.save(f\"{PLOTS}/{algorythm}/size/knn_accuracy_boxplot.pdf\", dpi=600, verbose = False)\n",
    "print(knn_fmeasure_plot)\n",
    "knn_fmeasure_plot.save(f\"{PLOTS}/{algorythm}/size/knn_fmeasure_boxplot.pdf\", dpi=600, verbose = False)\n",
    "knn_means_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN-DTW macierz pomyłek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = knn_df.copy()\n",
    "result = {}\n",
    "for dataset in datasets:    \n",
    "    testing_df[\"closest_acc\"] = np.abs(testing_df[testing_df.dataset == dataset].accuracy_score - knn_means_df[knn_means_df.index == dataset].accuracy_score[0])\n",
    "    local_min = testing_df[testing_df.dataset == dataset].closest_acc.min()\n",
    "    results_df = testing_df.loc[testing_df.closest_acc == local_min].head(1).copy()\n",
    "    result[dataset] = results_df\n",
    "    titles_options = [\n",
    "        (\"Macierz pomyłek:,\\n\"+\"algorytm KNN-DTW,\\n\" +f\"funkcja {dataset}\", None)\n",
    "    ]\n",
    "    for title, normalize in titles_options:\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(\n",
    "            results_df.y_true.iloc[0],\n",
    "            results_df.y_predict.iloc[0],\n",
    "            display_labels=class_names,\n",
    "            cmap=plt.cm.Blues,\n",
    "            normalize=normalize\n",
    "        )\n",
    "        disp.ax_.set_title(title)\n",
    "        disp.ax_.set_ylabel(\"Faktyczna klasa\")\n",
    "        disp.ax_.set_xlabel(\"Przewidzana klasa\")\n",
    "        if not os.path.exists(f\"{PLOTS}/{algorythm}\"):\n",
    "            os.makedirs(f\"{PLOTS}/{algorythm}\")\n",
    "        if normalize:\n",
    "            normalized = \"_normalized\"\n",
    "        else:\n",
    "            normalized = \"\"\n",
    "        disp.figure_.savefig(f\"{PLOTS}/{algorythm}/size/{dataset}{normalized}_confusion_matrix.pdf\", dpi=600, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza porównawcza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_means_df[\"algorytm\"] = \"KNN\"\n",
    "rocket_means_df[\"algorytm\"] = \"ROCKET\"\n",
    "hive_means_df[\"algorytm\"] = \"HIVECOTEV2\"\n",
    "\n",
    "dodge_text = position_dodge(width=0.4, preserve=\"single\")                              # new\n",
    "summary_df = pd.concat([knn_means_df, rocket_means_df, hive_means_df]).reset_index()\n",
    "acc_comp_graph = (ggplot(summary_df, aes(x='algorytm', y='accuracy_score', color = 'factor(dataset)'))\n",
    "    + guides(color=guide_legend(title='dataset'), fill='factor(dataset)')\n",
    "    + geom_point(size = 3, position=dodge_text)\n",
    "    + lims(y=(0.25, 1))  \n",
    "    + labs(title='Porównanie trafności między róznymi algorytmami')    \n",
    ")\n",
    "\n",
    "f1_comp_graph = (ggplot(summary_df, aes(x='algorytm', y='f1_measure', color = 'factor(dataset)'))\n",
    "    + guides(color=guide_legend(title='dataset'), fill='factor(dataset)')\n",
    "    + geom_point(size = 3, position=dodge_text)\n",
    "    + lims(y=(0.25, 1))  \n",
    "    + labs(title='Porównanie wartości F-miary między róznymi algorytmami')    \n",
    ")\n",
    "\n",
    "time_df = summary_df.groupby(\"algorytm\")[\"execution_time\"].mean().reset_index()\n",
    "time_df[\"execution_time\"] = time_df[\"execution_time\"].apply(lambda x : x/60)\n",
    "time_comp_graph = (ggplot(time_df, aes(x='algorytm', y='execution_time'))\n",
    "    + geom_point()\n",
    "    + labs(title='Porównanie wartości czasu przetwarzania między róznymi algorytmami') \n",
    "    + ylab(\"czas przetwarzania(minuty)\")\n",
    ")\n",
    "\n",
    "print(acc_comp_graph)\n",
    "acc_comp_graph.save(f\"{PLOTS}/size_acc_comp_graph.pdf\", dpi=600, verbose = False)\n",
    "print(f1_comp_graph)\n",
    "f1_comp_graph.save(f\"{PLOTS}/size_f1_comp_graph.pdf\", dpi=600, verbose = False)\n",
    "print(time_comp_graph)\n",
    "time_comp_graph.save(f\"{PLOTS}/size_time_comp_graph.pdf\", dpi=600, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(summary_df, values='accuracy_score', index='dataset', columns='algorytm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(summary_df, values='f1_measure', index='dataset', columns='algorytm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.groupby(\"algorytm\")[\"execution_time\"].mean().apply(lambda x : x/60).reset_index()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7d812b6b0ae132ddf009a87a46d9f35cc0a6e0343f7e24aabc62c05919a2e18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
