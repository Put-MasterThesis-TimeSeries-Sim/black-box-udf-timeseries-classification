{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie zbiorów danych testowych i treningowych\n",
    "\n",
    "Notatnik przygotowujący danie z folderu Oraginzed do eksperymentów. Dzieli on zbiór na testowy i treningowy i zapisuje w formacie, który jest gotowy do odczytu przez wybraną biblioteke sktime.\n",
    "Dodatkowo zapisuje oryginalny zbiór danych w csv do eksperymentów związanych z badaniem podobieństwa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przydatne funkcje i importy bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "from os import makedirs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.datatypes._panel._convert import (\n",
    "    from_multi_index_to_nested,\n",
    ")\n",
    "DATA_PATH = \"MachineLearning/ts_datasets\"\n",
    "seed = 42\n",
    "udf_types = ['aggregation', 'filtration', 'filtration-aggregation', 'filtration-aggregation-join', 'filtration-join']\n",
    "\n",
    "def remap_labels(label):\n",
    "    \"\"\"Mapuje ciąg znaków na liczby dla etykiet\n",
    "\n",
    "    Args:\n",
    "        label string: etykieta do zmapowania\n",
    "\n",
    "    Returns:\n",
    "        int: zmapowane nazwy funkcji na liczby:\n",
    "            0 - aggregation\n",
    "            1 - filtration\n",
    "            2 - filtration-aggregation\n",
    "            3 - filtration-aggregation-join\n",
    "            4 - flitration-join\n",
    "    \"\"\"\n",
    "    if label == \"aggregation\":\n",
    "        return 0\n",
    "    elif label == \"filtration\":\n",
    "        return 1\n",
    "    elif label == \"filtration-aggregation\":\n",
    "        return 2\n",
    "    elif label == \"filtration-aggregation-join\":\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def grow_snapshots(snapshot, label, snapshots):\n",
    "    \"\"\"Grow snapshot numbers incrementally for joined dataframe.\n",
    "\n",
    "    Args:\n",
    "        snapshot int: original snapshot number\n",
    "        label string: label of the snapshot\n",
    "        snapshots int: number of snapshots in each class.\n",
    "\n",
    "    Returns:\n",
    "        int: increased snapshot.\n",
    "    \"\"\"\n",
    "    if label == \"aggregation\":\n",
    "        return snapshot\n",
    "    elif label == \"filtration\":\n",
    "        return snapshot + snapshots\n",
    "    elif label == \"filtration-aggregation\":\n",
    "        return snapshot + 2*snapshots\n",
    "    elif label == \"filtration-aggregation-join\":\n",
    "        return snapshot + 3*snapshots\n",
    "    else:\n",
    "        return snapshot + 4*snapshots\n",
    "\n",
    "def read_dataset_for_sktime(udf_types, organised_directory, num_of_samples = 0, include_RAM = True, file_name = \"Time_series_udf_dataset.csv\", seed = 42):\n",
    "    random.seed(seed)\n",
    "    full_df = pd.read_csv(f\"./../{organised_directory}/{file_name}\")\n",
    "    max_snapshot = full_df[\"snapshot\"].max()\n",
    "\n",
    "    if num_of_samples > 0:\n",
    "        result_df = pd.DataFrame()\n",
    "        for udf_type in udf_types:\n",
    "            partial_df = full_df.loc[full_df.label == udf_type].copy()\n",
    "            samples = random.sample(range(partial_df.snapshot.max()), num_of_samples)\n",
    "            test_df = partial_df.loc[partial_df.snapshot.isin(samples)].copy()\n",
    "            result_df = pd.concat([result_df, test_df])\n",
    "        full_df = result_df\n",
    "\n",
    "    ts_y = full_df[full_df.epoch == 0.0].label.apply(remap_labels).to_numpy()\n",
    "    full_df[\"snapshot\"] = full_df.apply(lambda x: grow_snapshots(x.snapshot, x.label, max_snapshot), axis=1)\n",
    "    original_df = full_df.copy()\n",
    "\n",
    "    df = full_df.set_index([\"snapshot\", full_df.groupby(\"snapshot\").cumcount()])\n",
    "    index = pd.MultiIndex.from_product(df.index.levels, names=df.index.names)\n",
    "    output = df.reindex(index, fill_value=0).reset_index(level=1, drop=True).reset_index()\n",
    "    output[\"row_number\"] = output.groupby(\"snapshot\").cumcount()\n",
    "\n",
    "    if include_RAM:\n",
    "        ts_x = output[[\"snapshot\", \"row_number\", \"CPU\", \"RAM\"]].set_index([\"snapshot\", \"row_number\"])\n",
    "    else:\n",
    "        ts_x = output[[\"snapshot\", \"row_number\", \"CPU\"]].set_index([\"snapshot\", \"row_number\"])\n",
    "\n",
    "    return from_multi_index_to_nested(ts_x), ts_y, original_df\n",
    "\n",
    "def write_ts_file(base_path, dataset_name, possible_labels, X_train, X_test, y_train, y_test, base_df):\n",
    "    \"\"\"Writes file in ts format that is easy to read for tslearn library.\n",
    "\n",
    "    Args:\n",
    "        base_path string: base path for the dataset files\n",
    "        dataset_name string: name of the dataset\n",
    "        possible_labels string: list of possible labels delimited with space\n",
    "        X_train pandas.dataframe: X for train dataset\n",
    "        X_test pandas.dataframe: X for test dataset\n",
    "        y_train numpy.array: y for train dataset\n",
    "        y_test numpy.array: y for test dataset\n",
    "    \"\"\"\n",
    "    if not exists(f\"./../{base_path}/{dataset_name}\"):\n",
    "        makedirs(f\"./../{base_path}/{dataset_name}\")\n",
    "    text_file = open(f\"./../{base_path}/{dataset_name}/{dataset_name}_TRAIN.ts\", \"w\")\n",
    "    header = f\"@problemName {dataset_name}\\n@timeStamps false\\n@classLabel true {possible_labels}\\n@univariate false\\n@data\\n\"\n",
    "    text_file.write(header)\n",
    "    i = 0\n",
    "    for _, dataset in X_train.iterrows():\n",
    "        ram = ','.join(str(e) for e in dataset.RAM.to_list())\n",
    "        cpu = ','.join(str(e) for e in dataset.CPU.to_list())\n",
    "        label = y_train[i]\n",
    "        i += 1\n",
    "        output = cpu + ':' + ram + ':' + str(label) + \"\\n\"\n",
    "        text_file.write(output)\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(f\"./../{base_path}/{dataset_name}/{dataset_name}_TEST.ts\", \"w\")\n",
    "    text_file.write(header)\n",
    "    i = 0\n",
    "    for _, dataset in X_test.iterrows():\n",
    "        ram = ','.join(str(e) for e in dataset.RAM.to_list())\n",
    "        cpu = ','.join(str(e) for e in dataset.CPU.to_list())\n",
    "        label = y_test[i]\n",
    "        i += 1\n",
    "        output = cpu + ':' + ram + ':' + str(label) + \"\\n\"\n",
    "        text_file.write(output)\n",
    "    text_file.close()\n",
    "    original_df.loc[original_df[\"snapshot\"].isin(X_train.index)].copy().to_csv(f\"./../{base_path}/{dataset_name}/{dataset_name}_TRAIN.csv\", index = False)\n",
    "    original_df.loc[original_df[\"snapshot\"].isin(X_train.index)].copy().to_csv(f\"./../{base_path}/{dataset_name}/{dataset_name}_TEST.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stworzenie zbiorów danych w formacie ts i csv\n",
    "Domyślnie ustawiono liczbe próbek na 300, aby zachować balans danych (filtracja ma tylko 300 próbek)\n",
    "\n",
    "Wszystkie pliki lądują w folderze MachineLearning/ts_datasets/<nazwa_zbioru> z podziałem na test i train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default - zbiór danych z pliku Time_series_udf_dataset.csv, który zawiera nieprzetworzone dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime(udf_types[:3], \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Default\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(f\"./../Organised/Time_series_udf_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.groupby([\"snapshot\", \"label\"]).CPU.count().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized - zbiór danych z pliku Time_series_udf_dataset_normalized.csv, który zawiera dane po normalizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime(udf_types[:3], \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset_normalized.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Normalized\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default_smooth - zbiór danych z pliku Time_series_udf_dataset_smooth_6.csv, który zawiera wygładzone nieprzetworzone dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime(udf_types[:3], \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset_smooth_6.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Default_smooth\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized_smooth - zbiór danych z pliku Time_series_udf_dataset_normalized_smooth_6.csv, który zawiera wygładzone znormalizowane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime(udf_types[:3], \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset_normalized_smooth_6.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Normalized_smooth\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7d812b6b0ae132ddf009a87a46d9f35cc0a6e0343f7e24aabc62c05919a2e18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
