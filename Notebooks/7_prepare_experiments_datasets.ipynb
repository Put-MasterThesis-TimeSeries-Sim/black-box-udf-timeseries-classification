{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie zbiorów danych testowych i treningowych\n",
    "\n",
    "Notatnik przygotowujący danie z folderu Oraginzed do eksperymentów. Dzieli on zbiór na testowy i treningowy i zapisuje w formacie, który jest gotowy do odczytu przez wybraną biblioteke sktime.\n",
    "Dodatkowo zapisuje oryginalny zbiór danych w csv do eksperymentów związanych z badaniem podobieństwa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przydatne funkcje i importy bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "from os import makedirs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.datatypes._panel._convert import (\n",
    "    from_multi_index_to_nested,\n",
    ")\n",
    "DATA_PATH = \"MachineLearning/ts_datasets\"\n",
    "seed = 42\n",
    "udf_types = ['aggregation', 'filtration', 'filtration-aggregation', 'filtration-aggregation-join', 'filtration-join']\n",
    "samples = { \n",
    "    \"aggregation\" : 300,\n",
    "    \"filtration\" : 150,\n",
    "    \"filtration-aggregation\": 300,\n",
    "    \"filtration-aggregation-join\": 300,\n",
    "    \"filtration-join\": 300\n",
    "    }\n",
    "    \n",
    "def remap_labels(label):\n",
    "    \"\"\"Mapuje ciąg znaków na liczby dla etykiet\n",
    "\n",
    "    Args:\n",
    "        label string: etykieta do zmapowania\n",
    "\n",
    "    Returns:\n",
    "        int: zmapowane nazwy funkcji na liczby:\n",
    "            0 - aggregation\n",
    "            1 - filtration\n",
    "            2 - filtration-aggregation\n",
    "            3 - filtration-aggregation-join\n",
    "            4 - filtration-join\n",
    "    \"\"\"\n",
    "    if label == \"aggregation\":\n",
    "        return 0\n",
    "    elif label == \"filtration\":\n",
    "        return 1\n",
    "    elif label == \"filtration-aggregation\":\n",
    "        return 2\n",
    "    elif label == \"filtration-aggregation-join\":\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def grow_snapshots(snapshot, label, max_snapshots):\n",
    "    \"\"\"Inkrementuje snapshoty, w celu połączenia ich w rosnący ciąg\n",
    "\n",
    "    Args:\n",
    "        snapshot int: oryginalny snapshot\n",
    "        label str: etykieta od snapshotu\n",
    "        max_snapshots int: najwiekszy znapshot\n",
    "\n",
    "    Returns:\n",
    "        int: Powiększony snapshot.\n",
    "    \"\"\"\n",
    "    if label == \"aggregation\":\n",
    "        return snapshot\n",
    "    elif label == \"filtration\":\n",
    "        return snapshot + max_snapshots\n",
    "    elif label == \"filtration-aggregation\":\n",
    "        return snapshot + 2*max_snapshots\n",
    "    elif label == \"filtration-aggregation-join\":\n",
    "        return snapshot + 3*max_snapshots\n",
    "    else:\n",
    "        return snapshot + 4*max_snapshots\n",
    "\n",
    "def read_dataset_for_sktime_udf(udf_types, organised_directory, num_of_samples = 0, include_RAM = True, file_name = \"Time_series_udf_dataset.csv\", seed = 42):\n",
    "    \"\"\"Odczytuje plik z danymi i tworzy odpowiedni format do zapisu jako dataset\n",
    "\n",
    "    Args:\n",
    "        udf_types list: lista typów udf\n",
    "        organised_directory string: Nazwa folderu Organised\n",
    "        num_of_samples (int, optional): wielkość datasetów. Domyślnie 0.\n",
    "        include_RAM (bool, optional): czy bierzemy pod uwagę ram. Domyślnie True.\n",
    "        file_name (str, optional): nazwa pliku źródłowego. Domyślnie \"Time_series_udf_dataset.csv\".\n",
    "        seed (int, optional): stan losowosci. Domyślnie 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: dataset, etykiety datasetu, oryginalny format do zapisu\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    full_df = pd.read_csv(f\"./../{organised_directory}/{file_name}\")\n",
    "    max_snapshot = full_df[\"snapshot\"].max()\n",
    "    if num_of_samples > 0:\n",
    "        result_df = pd.DataFrame()\n",
    "        for udf_type in udf_types:\n",
    "            partial_df = full_df.loc[full_df.label == udf_type].copy()\n",
    "            samples = random.sample(range(partial_df.snapshot.max()), num_of_samples)\n",
    "            test_df = partial_df.loc[partial_df.snapshot.isin(samples)].copy()\n",
    "            result_df = pd.concat([result_df, test_df])\n",
    "        full_df = result_df\n",
    "\n",
    "    ts_y = full_df[full_df.epoch == 0.0].label.apply(remap_labels).to_numpy()\n",
    "    \n",
    "    labels = full_df[full_df.epoch == 0.0].label.to_numpy()\n",
    "    udf = full_df[full_df.epoch == 0.0].udf.to_numpy()\n",
    "    size = full_df[full_df.epoch == 0.0][[\"size\"]].to_numpy()\n",
    "\n",
    "    full_df[\"snapshot\"] = full_df.apply(lambda x: grow_snapshots(x.snapshot, x.label, max_snapshot), axis=1)\n",
    "\n",
    "\n",
    "    df = full_df.set_index([\"snapshot\", full_df.groupby(\"snapshot\").cumcount()])\n",
    "    index = pd.MultiIndex.from_product(df.index.levels, names=df.index.names)\n",
    "    output = df.reindex(index, fill_value=0).reset_index(level=1, drop=True).reset_index()\n",
    "    original_df = output.copy()\n",
    "\n",
    "    output[\"row_number\"] = output.groupby(\"snapshot\").cumcount()\n",
    "    \n",
    "    if include_RAM:\n",
    "        ts_x = output[[\"snapshot\", \"row_number\", \"CPU\", \"RAM\"]].set_index([\"snapshot\", \"row_number\"])\n",
    "    else:\n",
    "        ts_x = output[[\"snapshot\", \"row_number\", \"CPU\"]].set_index([\"snapshot\", \"row_number\"])\n",
    "\n",
    "    original_df = ts_x.copy()\n",
    "\n",
    "    max_length = original_df.groupby(\"snapshot\")[\"CPU\"].count().max()\n",
    "    original_df[\"label\"] = np.repeat(labels, max_length)\n",
    "    original_df[\"udf\"] = np.repeat(udf, max_length)\n",
    "    original_df[\"size\"] = np.repeat(size, max_length)\n",
    "\n",
    "    return from_multi_index_to_nested(ts_x), ts_y, original_df.reset_index()[[\"snapshot\", \"label\", \"udf\", \"row_number\", \"CPU\", \"RAM\", \"size\"]]\n",
    "\n",
    "def read_dataset_for_sktime_size(udf_type, organised_directory, num_of_samples = 0, include_RAM = True, file_name = \"Time_series_udf_dataset.csv\", seed = 42):\n",
    "    \"\"\"Odczytuje plik z danymi i tworzy odpowiedni format do zapisu jako dataset od eksperymentów wolumenów danych\n",
    "\n",
    "    Args:\n",
    "        udf_type str: nazwa typu\n",
    "        organised_directory str: Nazwa folderu Organised\n",
    "        num_of_samples (int, optional): wielkość datasetów. Domyślnie 0.\n",
    "        include_RAM (bool, optional): czy bierzemy pod uwagę ram. Domyślnie True.\n",
    "        file_name (str, optional): nazwa pliku źródłowego. Domyślnie \"Time_series_udf_dataset.csv\".\n",
    "        seed (int, optional): stan losowosci. Domyślnie 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: dataset, etykiety datasetu, oryginalny format do zapisu\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    full_df = pd.read_csv(f\"./../{organised_directory}/{file_name}\")\n",
    "    type_df = full_df.loc[full_df.label == udf_type].copy()\n",
    "\n",
    "    sizes = [1,2]\n",
    "    if num_of_samples > 0:\n",
    "        result_df = pd.DataFrame()\n",
    "        for size in sizes:\n",
    "            partial_df = type_df.loc[type_df[\"size\"] == size].copy()\n",
    "            samples = random.sample(list(set(partial_df.snapshot.to_numpy())), num_of_samples)\n",
    "            test_df = partial_df.loc[partial_df.snapshot.isin(samples)].copy()\n",
    "            result_df = pd.concat([result_df, test_df])\n",
    "        full_df = result_df\n",
    "    \n",
    "    ts_y = full_df[full_df.epoch == 0.0][\"size\"].to_numpy()\n",
    "\n",
    "    labels = full_df[full_df.epoch == 0.0].label.to_numpy()\n",
    "    udf = full_df[full_df.epoch == 0.0].udf.to_numpy()\n",
    "    size = full_df[full_df.epoch == 0.0][[\"size\"]].to_numpy()\n",
    "\n",
    "    df = full_df.set_index([\"snapshot\", full_df.groupby(\"snapshot\").cumcount()])\n",
    "    index = pd.MultiIndex.from_product(df.index.levels, names=df.index.names)\n",
    "    output = df.reindex(index, fill_value=0).reset_index(level=1, drop=True).reset_index()\n",
    "    output[\"row_number\"] = output.groupby(\"snapshot\").cumcount()\n",
    "\n",
    "    if include_RAM:\n",
    "        ts_x = output[[\"snapshot\", \"row_number\", \"CPU\", \"RAM\"]].set_index([\"snapshot\", \"row_number\"])\n",
    "    else:\n",
    "        ts_x = output[[\"snapshot\", \"row_number\", \"CPU\"]].set_index([\"snapshot\", \"row_number\"])\n",
    "        \n",
    "    original_df = ts_x.copy()\n",
    "\n",
    "    max_length = original_df.groupby(\"snapshot\")[\"CPU\"].count().max()\n",
    "    original_df[\"label\"] = np.repeat(labels, max_length)\n",
    "    original_df[\"udf\"] = np.repeat(udf, max_length)\n",
    "    original_df[\"size\"] = np.repeat(size, max_length)\n",
    "\n",
    "    return from_multi_index_to_nested(ts_x), ts_y, original_df.reset_index()[[\"snapshot\", \"label\", \"udf\", \"row_number\", \"CPU\", \"RAM\", \"size\"]]\n",
    "\n",
    "\n",
    "def write_ts_file(base_path, dataset_name, possible_labels, X_train, X_test, y_train, y_test, base_df):\n",
    "    \"\"\"Zapisuje plik do formatu ts.\n",
    "\n",
    "    Args:\n",
    "        base_path str: bazowa ścieżka do pliku datasetu\n",
    "        dataset_name str: nazwa datasetu\n",
    "        possible_labels str: możliwe etykiety\n",
    "        X_train pandas.DataFrame: X dla zbioru treningowego\n",
    "        X_test pandas.DataFrame: X dla zbioru testowego\n",
    "        y_train numpy.array: y dla zbioru treningowego\n",
    "        y_test numpy.array: y dla zbioru testowego\n",
    "    \"\"\"\n",
    "    if not exists(f\"./../{base_path}/{dataset_name}\"):\n",
    "        makedirs(f\"./../{base_path}/{dataset_name}\")\n",
    "    text_file = open(f\"./../{base_path}/{dataset_name}/{dataset_name}_TRAIN.ts\", \"w\")\n",
    "    header = f\"@problemName {dataset_name}\\n@timeStamps false\\n@classLabel true {possible_labels}\\n@univariate false\\n@data\\n\"\n",
    "    text_file.write(header)\n",
    "    i = 0\n",
    "    for _, dataset in X_train.iterrows():\n",
    "        ram = ','.join(str(e) for e in dataset.RAM.to_list())\n",
    "        cpu = ','.join(str(e) for e in dataset.CPU.to_list())\n",
    "        label = y_train[i]\n",
    "        i += 1\n",
    "        output = cpu + ':' + ram + ':' + str(label) + \"\\n\"\n",
    "        text_file.write(output)\n",
    "    text_file.close()\n",
    "\n",
    "    text_file = open(f\"./../{base_path}/{dataset_name}/{dataset_name}_TEST.ts\", \"w\")\n",
    "    text_file.write(header)\n",
    "    i = 0\n",
    "    for _, dataset in X_test.iterrows():\n",
    "        ram = ','.join(str(e) for e in dataset.RAM.to_list())\n",
    "        cpu = ','.join(str(e) for e in dataset.CPU.to_list())\n",
    "        label = y_test[i]\n",
    "        i += 1\n",
    "        output = cpu + ':' + ram + ':' + str(label) + \"\\n\"\n",
    "        text_file.write(output)\n",
    "    text_file.close()\n",
    "    base_df.loc[base_df[\"snapshot\"].isin(X_train.index)].copy().to_csv(f\"./../{base_path}/{dataset_name}/{dataset_name}_TRAIN.csv\", index = False)\n",
    "    base_df.loc[base_df[\"snapshot\"].isin(X_test.index)].copy().to_csv(f\"./../{base_path}/{dataset_name}/{dataset_name}_TEST.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stworzenie zbiorów danych w formacie ts i csv\n",
    "Domyślnie ustawiono liczbe próbek na 300, aby zachować balans danych (filtracja ma tylko 300 próbek)\n",
    "\n",
    "Wszystkie pliki lądują w folderze MachineLearning/ts_datasets/<nazwa_zbioru> z podziałem na test i train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default - zbiór danych z pliku Time_series_udf_dataset.csv, który zawiera nieprzetworzone dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime_udf(udf_types, \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Default\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized - zbiór danych z pliku Time_series_udf_dataset_normalized.csv, który zawiera dane po normalizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime_udf(udf_types, \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset_normalized.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Normalized\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default_smooth - zbiór danych z pliku Time_series_udf_dataset_smooth_6.csv, który zawiera wygładzone nieprzetworzone dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime_udf(udf_types, \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset_smooth_6.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Default_smooth\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized_smooth - zbiór danych z pliku Time_series_udf_dataset_normalized_smooth_6.csv, który zawiera wygładzone znormalizowane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x, df_y, original_df = read_dataset_for_sktime_udf(udf_types, \"Organised\", num_of_samples = 300, file_name = \"Time_series_udf_dataset_normalized_smooth_6.csv\", seed = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "write_ts_file(DATA_PATH, \"Normalized_smooth\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane do eksperymentów związanyc z wolumenem danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for udf_type in udf_types:\n",
    "    df_x, df_y, original_df  = read_dataset_for_sktime_size(udf_type, \"Organised\", num_of_samples = samples[udf_type], file_name = \"Time_series_udf_dataset.csv\", seed = seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "    write_ts_file(f\"{DATA_PATH}/{udf_type}\", \"Default\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for udf_type in udf_types:\n",
    "    df_x, df_y, original_df  = read_dataset_for_sktime_size(udf_type, \"Organised\", num_of_samples = samples[udf_type], file_name = \"Time_series_udf_dataset_normalized.csv\", seed = seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "    write_ts_file(f\"{DATA_PATH}/{udf_type}\", \"Normalized\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for udf_type in udf_types:\n",
    "    df_x, df_y, original_df  = read_dataset_for_sktime_size(udf_type, \"Organised\", num_of_samples = samples[udf_type], file_name = \"Time_series_udf_dataset_smooth_6.csv\", seed = seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "    write_ts_file(f\"{DATA_PATH}/{udf_type}\", \"Default_smooth\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for udf_type in udf_types:\n",
    "    df_x, df_y, original_df  = read_dataset_for_sktime_size(udf_type, \"Organised\", num_of_samples = samples[udf_type], file_name = \"Time_series_udf_dataset_normalized_smooth_6.csv\", seed = seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.33, random_state=seed)\n",
    "\n",
    "    write_ts_file(f\"{DATA_PATH}/{udf_type}\", \"Normalized_smooth\", set(df_y), X_train, X_test, y_train, y_test, original_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7d812b6b0ae132ddf009a87a46d9f35cc0a6e0343f7e24aabc62c05919a2e18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
