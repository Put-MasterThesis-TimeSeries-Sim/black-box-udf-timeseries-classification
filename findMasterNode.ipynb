{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from plotnine import *\n",
    "from plotnine.data import mpg\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_directory = \"Raw\"\n",
    "prepared_directory = \"Prepared\"\n",
    "organised_directory = \"Organised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sums(file_path):\n",
    "\n",
    "    node_data_df = pd.read_csv(file_path, dtype = {'timestamp' : 'string', 'PID' : 'int', 'CPU': 'float64', 'RAM': 'float64'})\n",
    "    node_data_df['timestamp'] = node_data_df['timestamp'].apply(lambda x: x if len(x.split(\".\")) > 1 else x + \".000000\" )\n",
    "    node_data_df['epoch'] = node_data_df['timestamp'].apply(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\") - datetime(1970, 1, 1)).total_seconds())\n",
    "    min_timestamp = node_data_df['epoch'].min()\n",
    "    node_data_df['epoch'] = node_data_df['epoch'].apply(lambda x: x - min_timestamp)\n",
    "    node_data_df.drop('PID', axis='columns', inplace=True)\n",
    "    node_data_df.drop('timestamp', axis='columns', inplace=True)\n",
    "    node_data_df = node_data_df.groupby(\"epoch\").sum()\n",
    "\n",
    "    return node_data_df\n",
    "\n",
    "\n",
    "for root, _, files in os.walk(f\".\\{entry_directory}\"):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            if re.search(\"\\d\\d_\\d\\d_\\d\\d\\d\\d_\\d\\d_\\d\\d_\\d\\d.csv$\", full_path):\n",
    "                if os.path.exists(full_path.replace(entry_directory, prepared_directory)):\n",
    "                    continue\n",
    "                calculated_dataframe_to_save = calculate_sums(full_path)\n",
    "                if not os.path.exists(root.replace(entry_directory, prepared_directory)):\n",
    "                    os.makedirs(root.replace(entry_directory, prepared_directory))\n",
    "                calculated_dataframe_to_save.to_csv(full_path.replace(entry_directory, prepared_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root = \".\\Prepared\"\n",
    "for functionTypeDirectory in os.listdir(root):\n",
    "    if os.path.isdir(os.path.join(root, functionTypeDirectory)):\n",
    "        for functionDirectory in os.listdir(os.path.join(root, functionTypeDirectory)):\n",
    "            for numberOfNodesDirectory in os.listdir(os.path.join(root, functionTypeDirectory, functionDirectory)):\n",
    "                destinationPath =  os.path.join(root, functionTypeDirectory, functionDirectory, numberOfNodesDirectory, 'source-data')\n",
    "                for nodeDirectory in os.listdir(destinationPath):\n",
    "                    path =  os.path.join(destinationPath, nodeDirectory)\n",
    "                    all_files = glob.glob(path + \"/*.csv\")\n",
    "                    dfs = []\n",
    "                    i = 1\n",
    "                    for filename in all_files:\n",
    "                        df = pd.read_csv(filename)\n",
    "                        dfs.append(df)\n",
    "                        if df['RAM'].mean() >= 2 and df['CPU'].head(15).mean() > 3:\n",
    "                            if not os.path.exists(os.path.join(destinationPath.replace(\"Prepared\", \"Organised\"), str(i))):\n",
    "                                os.makedirs(os.path.join(destinationPath.replace(\"Prepared\", \"Organised\"), str(i)))\n",
    "                            shutil.copyfile(filename, os.path.join(destinationPath.replace(\"Prepared\", \"Organised\"), str(i), os.path.basename(filename)))\n",
    "                        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(entry_dir, category):\n",
    "    directory = f\"./{entry_dir}/{category}\"\n",
    "    udf_dirs = os.listdir(directory)\n",
    "    label = category\n",
    "    result_df = pd.DataFrame()\n",
    "    snapshot = 0\n",
    "    for udf in udf_dirs:\n",
    "        for root, _, files in os.walk(f\"./{directory}/{udf}\"):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                node_data_df = pd.read_csv(full_path)\n",
    "                node_data_df['snapshot'] = snapshot\n",
    "                snapshot += 1\n",
    "                node_data_df[\"label\"] = label\n",
    "                node_data_df[\"udf\"] = udf\n",
    "                result_df = pd.concat([result_df,node_data_df])\n",
    "    result_df[[\"snapshot\", \"label\", \"udf\", \"epoch\", \"CPU\", \"RAM\"]].to_csv(f\"{directory}/joined_{category}.csv\", index = False)\n",
    "\n",
    "labels = ['aggregation', 'filtration', 'filtration-aggregation', 'filtration-aggregation-join', 'filtration-join']\n",
    "\n",
    "for label in labels:\n",
    "    join_files(organised_directory, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_values(entry_dir, category, window_value,):\n",
    "    # reading file\n",
    "    path_to_df = f\"./{entry_dir}/{category}/joined_{category}.csv\"\n",
    "    df = pd.read_csv(path_to_df)\n",
    "    cpu_df = df.groupby('snapshot')['CPU'].rolling(window=window_value, min_periods = 1).mean().to_frame()\n",
    "    ram_df = df.groupby('snapshot')['RAM'].rolling(window=window_value, min_periods = 1).mean().to_frame()\n",
    "    df['CPU'] = cpu_df.reset_index()['CPU'] \n",
    "    df['CPU'] = df['CPU'].round(2)\n",
    "    df['RAM'] = ram_df.reset_index()['RAM']\n",
    "    df['RAM'] = df['RAM'].round(2)\n",
    "    df['epoch'] = df['epoch'].round(3)\n",
    "    df.fillna(0).to_csv(f\"./{entry_dir}/{category}/{window_value}_smooth_{category}.csv\", index = False)\n",
    "\n",
    "for label in labels:\n",
    "    smooth_values(organised_directory, label, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "joined_aggregation = pd.read_csv(\"Organised/aggregation/joined_aggregation.csv\")\n",
    "joined_aggregation = joined_aggregation[joined_aggregation[\"snapshot\"] == 1]\n",
    "\n",
    "#(ggplot(joined_aggregation)         # defining what data to use\n",
    "#  + aes(x='epoch', y = \"CPU\")    # defining what variable to use\n",
    "#  + geom_line(size=0.5, color = \"red\") # defining the type of plot to use\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_aggregation_smooth = pd.read_csv(\"Organised/aggregation/6_smooth_aggregation.csv\")\n",
    "joined_aggregation_smooth = joined_aggregation_smooth[joined_aggregation_smooth[\"snapshot\"] == 1]\n",
    "\n",
    "(ggplot(joined_aggregation_smooth)         # defining what data to use\n",
    " + aes(x='epoch', y = \"CPU\")    # defining what variable to use\n",
    " + geom_line(size=0.5, color = \"red\") # defining the type of plot to use\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
